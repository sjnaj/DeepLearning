{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[softmax](https://zhuanlan.zhihu.com/p/105722023)\n",
    "\n",
    "Softmax从字面上来说，可以分成soft和max两个部分。max故名思议就是最大值的意思。Softmax的核心在于soft，而soft有软的含义，与之相对的是hard硬。很多场景中需要我们找出数组所有元素中值最大的元素，实质上都是求的hardmax\n",
    "\n",
    "<img src=\"https://www.zhihu.com/equation?tex=Softmax%28z_%7Bi%7D%29%3D%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D%7B%5Csum_%7Bc+%3D+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D%7D\"/>\n",
    "\n",
    "其中$z_i$为第i个节点的输出值，C为输出节点的个数，即分类的类别个数。通过Softmax函数就可以将多分类的输出值转换为范围在[0, 1]和为1的概率分布。\n",
    "\n",
    "优点：\n",
    "\n",
    "- 经过使用指数形式的Softmax函数能够将差距大的数值距离拉的更大。\n",
    "\n",
    "- 在深度学习中通常使用反向传播求解梯度进而使用梯度下降进行参数更新的过程，而指数函数在求导的时候比较方便。比如 [公式] 。\n",
    "\n",
    "缺点：\n",
    "\n",
    "- 当$z_i$值非常大的话，计算得到的数值也会变的非常大，数值可能会溢出。\n",
    "\n",
    "- 当然针对数值溢出有其对应的优化方法，将每一个输出值减去输出值中最大的值\n",
    "  \n",
    "  <img src=\"https://www.zhihu.com/equation?tex=softmax%28z_%7Bi%7D%29%3D%5Cfrac%7Be%5E%7Bz_%7Bi%7D+-+D%7D%7D%7B%5Csum_%7Bc+%3D+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D-D%7D%7D%7D\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax求导\n",
    "\n",
    "在对Softmax函数求导的时候，需要分两种情况考虑。即对第i个输出节点，分为对j=i的$z_j$求导以及其它$j\\neq i$的$z_j$求导。\n",
    "\n",
    "<img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+y_%7Bi%7D%7D%7B%5Cpartial+z_%7Bj%7D%7D+%3D+%5Cbegin%7Bcases%7D+p_%7Bi%7D%281+-+p_%7Bj%7D%29+%26+j+%3D+i+%5C%5C+-p_%7Bj%7D%5Ccdot+p_%7Bi%7D+%26+j%5Cne+i+%5C%5C+%5Cend%7Bcases%7D\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵损失函数\n",
    "\n",
    "添加log运算不影响函数的单调性，首先为$p_i$添加log运算：\n",
    "\n",
    "<img src=\"https://www.zhihu.com/equation?tex=log%5C+p_%7Bi%7D+%3D+log+%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D%7B%5Csum_%7Bc+%3D+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D%7D\">\n",
    "\n",
    "由于此时的$p_i$是正确类别对应的输出节点的概率，当然希望此时的 $p_i$越大越好（当然最大不能超过1）。通常情况下使用梯度下降法来迭代求解，因此只需要为$logp_i$加上一个负号变成损失函数，现在变成希望损失函数越小越好：\n",
    "\n",
    "<img src=\"https://www.zhihu.com/equation?tex=loss_%7Bi%7D++%3D+-log%5C+p_%7Bi%7D+%3D+-+log%5C+%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D%7B%5Csum_%7Bc+%3D+1%7D%5E%7BC%7D%7Be%5E%7Bz_%7Bc%7D%7D%7D%7D\">\n",
    "\n",
    "交叉熵\n",
    "\n",
    "<img src=\"https://www.zhihu.com/equation?tex=L+%3D+-+%5Csum_%7Bc+%3D+1%7D%5E%7BC%7D%7By_%7Bc%7D%5C+log%28p_%7Bc%7D%29%7D\">\n",
    "\n",
    "对于分类任务来说，真实的样本标签通常表示为one-hot的形式。比如对于三分类来说，真实类别的索引位置为1，也就是属于第二个类别，那么使用one-hot编码表示为[0, 1, 0]，也就是仅正确类别位置为1，其余位置都为0。而式子中的 $y_c$ 就是真实样本的标签值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数的导数\n",
    "\n",
    "<img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+z_%7Bi%7D%7D+%3D+p_%7Bi%7D+-+y_%7Bi%7D\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80fc50c49c0f82d72130c0f352c3673108d05a1ad36a5739d42510bc062d123a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('miniconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
