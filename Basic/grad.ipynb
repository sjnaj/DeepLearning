{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2., 3., 4.],\n",
       "       [5., 6., 7., 8., 9.]], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "A=torch.arange(10,dtype=torch.float32).reshape(2,5)\n",
    "B=A.clone()#类似于numpy的copy\n",
    "B[0][0]=10\n",
    "A\n",
    "import numpy as np\n",
    "A=np.arange(10,dtype=np.float32).reshape(2,5)\n",
    "B=A.copy()\n",
    "B[0][0]=10\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "        [0.2105, 0.2368, 0.2632, 0.2895]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=torch.arange(12).reshape(3,4)\n",
    "A\n",
    "sum_A=A.sum(axis=1,keepdims=True)#保持维度，便于广播\n",
    "A/sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  6,  8, 10],\n",
       "        [12, 15, 18, 21]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)#沿x轴累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 22., 38.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.ones(4,dtype=torch.float32)\n",
    "torch.dot(x,x+1)\n",
    "torch.mv(A.float(),x)#矩阵向量乘法，数据类型要一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 85,  80,  71],\n",
       "        [ 69,  87,  71],\n",
       "        [ 65,  46,  79],\n",
       "        [ 91,  57,  83],\n",
       "        [ 87, 106, 112]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randint(10,(5, 4))#10是上限\n",
    "B = torch.randint(10,(4, 3))\n",
    "torch.mm(A, B)#向量乘法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0221)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.randn(5))#范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵求导\n",
    "\n",
    "标量对向量(列)求导后得到的是行向量(分子布局，每一列的分子不变)\n",
    "\n",
    "列向量对标量求导的结果还是列向量\n",
    "\n",
    "列向量对列向量求导的结果是分子布局的矩阵（列数等于X的维数）\n",
    "\n",
    "pytorch中的一维数组是以列向量为数学计算约定，而已行向量为表示形式的向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动求导\n",
    "\n",
    "神经网络的训练过程中，前向传播和反向传播交替进行，前向传播通过训练数据和权重参数计算输出结果；反向传播通过导数链式法则计算损失函数对各参数的梯度，并根据梯度进行参数的更新\n",
    "\n",
    "求导是从外层向内层求的，需要存储之前计算的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.arange(4.)#必须是浮点型才能求导\n",
    "x.requires_grad_(True)#注意最后的下划线\n",
    "y=2*torch.dot(x,x)\n",
    "y\n",
    "y.backward()#用反向传播函数计算y关于x的梯度\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()#默认梯度会累积，重新计算需要清零\n",
    "y=x.sum()\n",
    "y\n",
    "y.backward()\n",
    "x.grad\n",
    "#因为y = x.sum() = (x[0][0] + x[0][1] + x[1][0] + x[1][1])\n",
    "#每个值的梯度都为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y=x*x#结果是向量，不能直接backward\n",
    "y.sum().backward()#求和转化为标量再求导([x0*x0,...]->x0*x0+...)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y=x*x\n",
    "u=y.detach()#设置u为固定常量\n",
    "z=u*x\n",
    "z.sum().backward()\n",
    "x.grad==u#导数为u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "控制流构建函数图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    b=a*2\n",
    "    while b.norm()<1000:\n",
    "        b=b*2\n",
    "    if b.sum()>0:\n",
    "        c=b\n",
    "    else:\n",
    "        c=100*b\n",
    "    return c\n",
    "a=torch.randn(size=(),requires_grad=True)#size为空产生标量，标准正态分布\n",
    "d=f(a)\n",
    "d.backward()\n",
    "a.grad==d/a\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e73c9050286a65e763d135ff6841e7640eefc6b75dcfcb460df3e503612b7f71"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('V3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
