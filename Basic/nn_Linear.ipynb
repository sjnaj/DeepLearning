{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.2320,  1.6485],\n",
       "         [ 0.9697,  0.2419],\n",
       "         [-1.0298,  0.0290],\n",
       "         [-1.4744,  0.1274],\n",
       "         [ 2.4440, -0.3168],\n",
       "         [ 0.6069, -0.9867],\n",
       "         [-0.7021,  0.2438],\n",
       "         [ 0.6589, -0.6764],\n",
       "         [-0.3180, -0.6541],\n",
       "         [-0.1464, -0.2012]]),\n",
       " tensor([[-0.9363],\n",
       "         [ 5.3264],\n",
       "         [ 2.0396],\n",
       "         [ 0.8222],\n",
       "         [10.1469],\n",
       "         [ 8.7513],\n",
       "         [ 1.9735],\n",
       "         [ 7.8127],\n",
       "         [ 5.7805],\n",
       "         [ 4.5785]])]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l\n",
    "\n",
    "true_w=torch.tensor([2,-3.4])\n",
    "true_b=4.2\n",
    "features,labels=d2l.synthetic_data(true_w,true_b,1000)\n",
    "\n",
    "dataset=data.TensorDataset(*(features,labels))#传入数据集元组列表得到dataset\n",
    "\n",
    "batch_size=10\n",
    "data_iter=data.DataLoader(dataset,batch_size,shuffle=True)#获取乱序数据迭代器\n",
    "\n",
    "next(iter(data_iter))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "net =nn.Sequential(nn.Linear(2,1))#将线性模型放在一个容器里，本例中可不使用容器\n",
    "#Applies a linear transformation to the incoming data: y = xA^T + b\n",
    "#2，1是x,y的维度,输入输出矩阵的行向量的长度,同时确定了w(A)和b的形状\n",
    "\n",
    "\n",
    "loss=nn.MSELoss()#mean squared error (squared L2 norm)\n",
    "#输入输出形状相同\n",
    "#The mean operation still operates over all the elements, and divides by n.\n",
    "# The division by n can be avoided if one sets reduction = 'sum'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate为：0.001时:\n",
      "epoch 1,loss0.000406\n",
      "epoch 2,loss0.000100\n",
      "epoch 3,loss0.000100\n",
      "w的估计误差：tensor([0.0008, 0.0003], grad_fn=<SubBackward0>)\n",
      "b的估计误差：tensor([6.8188e-05], grad_fn=<RsubBackward1>)\n",
      "learning rate为：0.01时:\n",
      "epoch 1,loss0.000432\n",
      "epoch 2,loss0.000100\n",
      "epoch 3,loss0.000100\n",
      "w的估计误差：tensor([-2.5749e-05,  3.5596e-04], grad_fn=<SubBackward0>)\n",
      "b的估计误差：tensor([-3.8147e-05], grad_fn=<RsubBackward1>)\n",
      "learning rate为：0.1时:\n",
      "epoch 1,loss0.000363\n",
      "epoch 2,loss0.000100\n",
      "epoch 3,loss0.000100\n",
      "w的估计误差：tensor([-0.0004,  0.0002], grad_fn=<SubBackward0>)\n",
      "b的估计误差：tensor([0.0006], grad_fn=<RsubBackward1>)\n",
      "learning rate为：1时:\n",
      "epoch 1,loss0.000397\n",
      "epoch 2,loss0.000101\n",
      "epoch 3,loss0.000100\n",
      "w的估计误差：tensor([-0.0003,  0.0006], grad_fn=<SubBackward0>)\n",
      "b的估计误差：tensor([-4.7684e-07], grad_fn=<RsubBackward1>)\n",
      "learning rate为：10时:\n",
      "epoch 1,loss0.000397\n",
      "epoch 2,loss0.000100\n",
      "epoch 3,loss0.000100\n",
      "w的估计误差：tensor([-1.0729e-05,  6.0010e-04], grad_fn=<SubBackward0>)\n",
      "b的估计误差：tensor([0.0005], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "lr_list = [0.001, 0.01, 0.1, 1, 10]\n",
    "for lr in lr_list:\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr=0.03)  # 传入网络参数w,b\n",
    "    # stochastic gradient descent\n",
    "    # net[0].reset_parameters()#默认初始化函数\n",
    "    net[0].weight.data.normal_(0, 0.01)  # 输入权重初值\n",
    "    net[0].bias.data.fill_(0)  # 偏差初值\n",
    "\n",
    "    print(f\"learning rate为：{lr}时:\")\n",
    "    num_epochs = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "            l = loss(net(X), y)\n",
    "            trainer.zero_grad()  # 清零梯度\n",
    "            l.backward()\n",
    "            trainer.step()  # 梯度下降更新参数\n",
    "        l = loss(net(features), labels)\n",
    "        print(f\"epoch {epoch+1},loss{l:f}\")\n",
    "    para_iter = net.parameters()\n",
    "    print(f\"w的估计误差：{true_w-next(iter(para_iter)).reshape(true_w.shape)}\")\n",
    "\n",
    "    print(f\"b的估计误差：{true_b-next(iter(para_iter))}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80fc50c49c0f82d72130c0f352c3673108d05a1ad36a5739d42510bc062d123a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('miniconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
